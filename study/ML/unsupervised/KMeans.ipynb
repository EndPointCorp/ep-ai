{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means Clustering\n",
    "\n",
    "k-Means clustering is an unsupervised learning algorithm. So initially we have no idea of the labels and how many cluster we'd have exactly. Let's say we have a N size of list of house prices. When you look at such a list of values without labels, at first you can think of calculating some statistical variables like mean, median, standard deviation etc. All these metrics make sense in terms of location and variance of the dataset. However we would need to look in a different way to cluster or group these house prices; in a simple way for example cheap houses and expensive houses groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's say each value in the list represents scales of $10K\n",
    "data_list = [10, 12, 22, 23, 25, 30, 40, 50, 80, 90, 120, 140]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Means clustering algorithm starts with deciding how many clusters we want. Let's say here we start with K = 2 clusters.\n",
    "\n",
    "1. Then we initialize K centroids randomly at the beginning. \n",
    "2. At this point we calculate the distance of every data point to each of centroids. \n",
    "3. After this calculation we can assign each data point to the closest centroid. And this way we have our initial clusters at the first calculation. \n",
    "4. Then we assign the centroids to the mean of its cluster's data.\n",
    "5. We repeat the steps 2-4 until `convergence`. When we reach the convergence the centroids stop changing which indicates the end of the training of our algorithm.\n",
    "The convergence in k-Means is called `quantization`.\n",
    "\n",
    "However the initial random centroid selection could end up with different clustering outcomes. So k-means converges to local minima.\n",
    "\n",
    "Please note that, I assumed the data points are 1-dimensional so the distance is simply calculated as the difference of the points. We can later update this algorithm for multi-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterDataPoint():\n",
    "    def __init__(self, _point, _centroid):\n",
    "        self._point = _point\n",
    "        self._centroid = _centroid\n",
    "\n",
    "    def update_centroid(self, _centroid):\n",
    "        self._centroid = _centroid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic algorithms suggest the convergence using an iteration count limit so that the big enough iteration count would help the convergence reached. In my algorithm I am going to use gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class KMeans():\n",
    "    def __init__(self, _X, _K):\n",
    "        self._X = _X\n",
    "        self._ClusterPoints = []\n",
    "        self._min = np.min(self._X)\n",
    "        self._max = np.max(self._X)\n",
    "        self._K = _K\n",
    "        self._centroids = []\n",
    "        self._clusters = []\n",
    "\n",
    "        self.initiate_centroids()\n",
    "\n",
    "\n",
    "    def calculate_distance(self, p1, p2):\n",
    "        return abs(p1 - p2)\n",
    "\n",
    "\n",
    "    def initiate_centroids(self):\n",
    "        for i in range(0, self._K):\n",
    "            rnumber = random.randint(self._min, self._max)\n",
    "            self._centroids[i] = rnumber\n",
    "\n",
    "        print(\"Generated initial random centroids: \", self._centroids)\n",
    "\n",
    "        # assign points to the initial centroids\n",
    "        self.assign_centroids()\n",
    "\n",
    "\n",
    "    def assign_centroids(self):\n",
    "        for p in self._X:\n",
    "            cdp = ClusterDataPoint(p, None)\n",
    "            minDist = -1\n",
    "            for c in self._centroids:\n",
    "                dist = self.calculate_distance(c, p)\n",
    "                if minDist == -1 or dist < minDist:\n",
    "                    minDist = dist\n",
    "                    cdp.update_centroid(c)\n",
    "            self._ClusterPoints.append(cdp)\n",
    "\n",
    "\n",
    "    def recalculate_centroids(self):\n",
    "        new_centroids = []\n",
    "        for c in self._centroids:\n",
    "            # filter cluster points for current centroid\n",
    "            cluster_points = [cp._point for cp in self._ClusterPoints if cp._centroid == c]\n",
    "            cluster_mean = np.mean(cluster_points)\n",
    "            new_centroids.append(cluster_mean)\n",
    "        self._centroids = new_centroids\n",
    "\n",
    "        self.assign_centroids()\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        # convergence\n",
    "        pass\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fc493fde361d7be51044bcee58ba069bf2d89e960dc3594d2539c90e57618d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('ds-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
